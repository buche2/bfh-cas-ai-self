{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "10 Frozen Lake HRK.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "10 Exercise Monte Carlo methods: ***Frozen Lake***\n",
    "==============\n",
    "\n",
    "**Author**: `Hansruedi Keller, 14.01.2022`"
   ],
   "metadata": {
    "id": "D-YfWge-yYtZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 0: Explore *Frozen Lake* Environment"
   ],
   "metadata": {
    "id": "IJTSptPWyyJJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\enrico\\anaconda3\\envs\\bfh\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\enrico\\anaconda3\\envs\\bfh\\lib\\site-packages (from gym) (1.22.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\enrico\\anaconda3\\envs\\bfh\\lib\\site-packages (from gym) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BaBW2OS2KEPT"
   },
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RwTOY-lnKFU0",
    "outputId": "b7e4fbb2-42b6-454f-b216-8d4830ac5258"
   },
   "source": [
    "# ======== ADJUST HERE AS APPROPRIATE ========\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "env.render()                     # What does the board look like"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ekDB1omKHpp",
    "outputId": "fab33ed0-7b24-459b-970b-877ad2eca0e6"
   },
   "source": [
    "print(env.observation_space)    # How many states are there?\n",
    "print(env.action_space)         # How many actions are there?"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "Discrete(4)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "**16 States**\n",
    "*   SFFF       (S: starting point, safe)\n",
    "*   FHFH       (F: frozen surface, safe)\n",
    "*   FFFH       (H: hole, fall to your doom)\n",
    "*   HFFG       (G: goal, where the frisbee is located)\n",
    "---\n",
    "\n",
    "**4 Actions**\n",
    "*   0\tMove Left\n",
    "*   1\tMove Down\n",
    "*   2\tMove Right\n",
    "*   3\tMove Up"
   ],
   "metadata": {
    "id": "nlYvhypjy8NX"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lk_9DXOmK1jM",
    "outputId": "0cee98b2-4a46-4fee-cd88-cbecd78a8929"
   },
   "source": [
    "# Show examples based on a random policy\n",
    "for i_episode in range(3):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        # print(state)\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        env.render();\n",
    "        if done:\n",
    "            print('End of Frozen Lake game! Reward: ', reward)\n",
    "            if reward == 1:\n",
    "              print('You reached the goal :)\\n')\n",
    "              break\n",
    "            if state == 5 or state == 7 or state == 11 or state == 12:\n",
    "               print('You fell into hole {} :-('.format(state))\n",
    "               break\n",
    "            print('You went too long, you will never find it. :-(')\n",
    "            break\n",
    "    print()"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FH\u001B[41mF\u001B[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001B[41mF\u001B[0mH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFF\u001B[41mH\u001B[0m\n",
      "HFFG\n",
      "End of Frozen Lake game! Reward:  0.0\n",
      "You fell into hole 11 :-(\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFF\u001B[41mF\u001B[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHF\u001B[41mH\u001B[0m\n",
      "FFFH\n",
      "HFFG\n",
      "End of Frozen Lake game! Reward:  0.0\n",
      "You fell into hole 7 :-(\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001B[41mH\u001B[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "End of Frozen Lake game! Reward:  0.0\n",
      "You fell into hole 5 :-(\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 1: MC Prediction\n",
    "\n",
    "The function accepts as **input**:\n",
    "- `fl_env`: This is an instance of OpenAI Gym's Frozen Lake environment.\n",
    "\n",
    "It returns as **output**:\n",
    "- `episode`: This is a list of (state, action, reward) tuples (of tuples) and corresponds to $(S_0, A_0, R_1, \\ldots, S_{T-1}, A_{T-1}, R_{T})$, where $T$ is the final time step.  In particular, `episode[i]` returns $(S_i, A_i, R_{i+1})$, and `episode[i][0]`, `episode[i][1]`, and `episode[i][2]` return $S_i$, $A_i$, and $R_{i+1}$, respectively."
   ],
   "metadata": {
    "id": "KbcScqBy0ZdN"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pSnFnhZsMY4K"
   },
   "source": [
    "def generate_episode_from_limit_stochastic(fl_env):\n",
    "    episode = []\n",
    "    state = fl_env.reset()\n",
    "    while True:\n",
    "        # Define probabilities =================================================\n",
    "        probs =[[0.5, 0.25, 0.0, 0.25], [0.25, 0.5, 0.25, 0.0], [0.0, 0.25, 0.5, 0.25], [0.25, 0.0, 0.25, 0.5]] \n",
    "        # Define actions =======================================================\n",
    "        action_choice = np.random.choice(np.arange(4))\n",
    "        action = np.random.choice(np.arange(4), p=probs[action_choice])\n",
    "        next_state, reward, done, info = fl_env.step(action)\n",
    "        # ======== ADJUST HERE AS APPROPRIATE ========\n",
    "        # print(\"State: {}, Action taken: {}, Next state: {}, Reward: {}\".format(state, action, next_state, reward))\n",
    "        # Record the walk\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            # ======== ADJUST HERE AS APPROPRIATE ========\n",
    "            # print('End of Frozen Lake tour')\n",
    "            if reward == 1:\n",
    "              print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "              print('Congratulation! You reached the goal :-)')\n",
    "              print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "            break\n",
    "    return episode"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Illustrate the procedure\n",
    "for i in range(2):\n",
    "    print(\"Episode =\", i)\n",
    "    print(generate_episode_from_limit_stochastic(env))\n",
    "    print()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tGYLtpxBGpkR",
    "outputId": "519fc03d-d204-45e6-9a86-7cb1891ad40c"
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode = 0\n",
      "[(0, 2, 0.0), (1, 3, 0.0), (1, 2, 0.0), (2, 0, 0.0), (1, 2, 0.0), (2, 3, 0.0), (2, 3, 0.0), (2, 0, 0.0), (1, 3, 0.0), (1, 2, 0.0), (2, 2, 0.0), (3, 3, 0.0), (3, 2, 0.0), (3, 1, 0.0)]\n",
      "\n",
      "Episode = 1\n",
      "[(0, 3, 0.0), (0, 3, 0.0), (0, 1, 0.0), (4, 2, 0.0)]\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    " Implementation of MC prediction with  arguments:\n",
    "- `env`: This is an instance of an OpenAI Gym environment.\n",
    "- `num_episodes`: This is the number of episodes that are generated through agent-environment interaction.\n",
    "- `generate_episode`: This is a function that returns an episode of interaction.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as output:\n",
    "- `Q`: This is a dictionary (of one-dimensional arrays) where `Q[s][a]` is the estimated action value corresponding to state `s` and action `a`."
   ],
   "metadata": {
    "id": "rqXsnKFaJEvA"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pCwSDAd4KWMr"
   },
   "source": [
    "def mc_prediction_q(env, num_episodes, generate_episode, gamma=.99):\n",
    "    # initialize empty dictionaries of arrays\n",
    "    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    N = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        # generate an episode (one game)\n",
    "        # starting with own policy, we need a constant policy for prediction\n",
    "        episode = generate_episode(env)\n",
    "        # obtain the states, actions, and rewards. Compiles list of states, actions, rewards (tuples)\n",
    "        states, actions, rewards = zip(*episode)\n",
    "        # prepare for discounting (calculate list of gammas)\n",
    "        discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
    "        \n",
    "        # print(\"discounts:\", discounts)\n",
    "        # print(\"rewards:\", rewards)\n",
    "        # print(\"states:\", states)\n",
    "        # update the sum of the returns, number of visits, and action-value \n",
    "        # function estimates for each state-action pair in the episode\n",
    "        # Update of Q-table afteer each episode (game)\n",
    "        for i, state in enumerate(states):\n",
    "            # print(i)\n",
    "            # print(\"Rewards[i:]:\", rewards[i:])\n",
    "            # print(\"Discounts[:-(1+i)]:\", discounts[:-(1+i)])\n",
    "            # print(\"----------------------\")\n",
    "            # Next line is MOST IMPORTANT!\n",
    "            returns_sum[state][actions[i]] += sum(rewards[i:]*discounts[:-(1+i)])\n",
    "            N[state][actions[i]] += 1.0\n",
    "            # MC prediction update, based on average\n",
    "            Q[state][actions[i]] = returns_sum[state][actions[i]] / N[state][actions[i]]\n",
    "    return Q"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yBZCrsJDMSxH",
    "outputId": "1e5b4298-944c-4304-a7f1-4d72fc9323aa"
   },
   "source": [
    "# Obtain the action-value function\n",
    "# ======== ADJUST HERE AS APPROPRIATE ========\n",
    "Q = mc_prediction_q(env, 1000, generate_episode_from_limit_stochastic)\n",
    "V = dict((k,np.max(v)) for k, v in Q.items())\n",
    "V"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Congratulation! You reached the goal :-)\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Congratulation! You reached the goal :-)\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Congratulation! You reached the goal :-)\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Congratulation! You reached the goal :-)\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Congratulation! You reached the goal :-)\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Congratulation! You reached the goal :-)\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Congratulation! You reached the goal :-)\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Congratulation! You reached the goal :-)\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Congratulation! You reached the goal :-)\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Congratulation! You reached the goal :-)\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Congratulation! You reached the goal :-)\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Congratulation! You reached the goal :-)\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Congratulation! You reached the goal :-)\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Congratulation! You reached the goal :-)\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Congratulation! You reached the goal :-)\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Episode 1000/1000."
     ]
    },
    {
     "data": {
      "text/plain": "{0: 0.019974824831129015,\n 4: 0.02151158034602996,\n 8: 0.06603272488220564,\n 9: 0.1153352189215092,\n 13: 0.5086480866545701,\n 10: 0.48787094029642275,\n 1: 0.04095017334201163,\n 2: 0.07152093895205101,\n 6: 0.23163155040873915,\n 3: 0.011493181280673956,\n 14: 1.0}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 2: MC Control\n",
    "\n",
    "In this section, you will write your own implementation of constant-$\\alpha$ MC control.  \n",
    "\n",
    "Your algorithm has four arguments:\n",
    "- `env`: This is an instance of an OpenAI Gym environment.\n",
    "- `num_episodes`: This is the number of episodes that are generated through agent-environment interaction.\n",
    "- `alpha`: This is the step-size parameter for the update step.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as output:\n",
    "- `Q`: This is a dictionary (of one-dimensional arrays) where `Q[s][a]` is the estimated action value corresponding to state `s` and action `a`.\n",
    "- `policy`: This is a dictionary where `policy[s]` returns the action that the agent chooses after observing state `s`.\n",
    "\n",
    "(_Feel free to define additional functions to help you to organize your code._)"
   ],
   "metadata": {
    "id": "cTzK6op-8r3s"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YPrMbvMmOupU"
   },
   "source": [
    "def generate_episode_from_Q(env, Q, epsilon, nA):\n",
    "    \"\"\" generates an episode from following the epsilon-greedy policy \"\"\"\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = np.random.choice(np.arange(nA), p=get_probs(Q[state], epsilon, nA)) \\\n",
    "                                    if state in Q else env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    return episode\n",
    "\n",
    "def get_probs(Q_s, epsilon, nA):                      # Policy evaluation\n",
    "    \"\"\" obtains the action probabilities corresponding to epsilon-greedy policy \"\"\"\n",
    "    policy_s = np.ones(nA) * epsilon / nA\n",
    "    best_a = np.argmax(Q_s)\n",
    "    policy_s[best_a] = 1 - epsilon + (epsilon / nA)\n",
    "    return policy_s\n",
    "\n",
    "def update_Q(env, episode, Q, alpha, gamma):          # Policy improvement\n",
    "    \"\"\" updates the action-value function estimate using the most recent episode \"\"\"\n",
    "    states, actions, rewards = zip(*episode)\n",
    "    # Prepare for discounting\n",
    "    discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
    "    for i, state in enumerate(states):\n",
    "        old_Q = Q[state][actions[i]] \n",
    "        # Calculate returns and write these into matrix, starting with gammma^0 \n",
    "        Q[state][actions[i]] = old_Q + alpha*(sum(rewards[i:]*discounts[:-(1+i)]) - old_Q)\n",
    "    #\n",
    "    # Q-values with index for state and index for action\n",
    "    return Q"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yADQQTYXOxWJ"
   },
   "source": [
    "def mc_control(env, num_episodes, alpha, gamma=1.0, eps_start=1.0, eps_decay=0.999, eps_min=0.05):\n",
    "    nA = env.action_space.n\n",
    "    # initialize empty dictionary of arrays\n",
    "    Q = defaultdict(lambda: np.zeros(nA))\n",
    "    epsilon = eps_start\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        # update the value of epsilon\n",
    "        epsilon = max(epsilon*eps_decay, eps_min)\n",
    "        # generate an episode by following epsilon-greedy policy\n",
    "        episode = generate_episode_from_Q(env, Q, epsilon, nA)\n",
    "        # update the action-value function estimate using the episode\n",
    "        Q = update_Q(env, episode, Q, alpha, gamma)\n",
    "    # determine the policy corresponding to the final action-value function estimate\n",
    "    policy = dict((k,np.argmax(v)) for k, v in Q.items())\n",
    "    return policy, Q"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PIpwNn_3Oz72",
    "outputId": "97546039-7749-4fef-f0bd-a540a02e6c50"
   },
   "source": [
    "# ======== ADJUST HERE AS APPROPRIATE ========\n",
    "# 1000+ episodes are required for successs\n",
    "policy, Q = mc_control(env, 10000, 0.02)"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10000/10000."
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RiuQ73dwO746",
    "outputId": "1b421b0d-f84d-4e12-d1a3-9aaac24fa846"
   },
   "source": [
    "# Show state values V\n",
    "V = dict((k,np.max(v)) for k, v in Q.items())\n",
    "V"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: 0.93329194353414,\n 1: 0.9521786556553145,\n 2: 0.9559570679604529,\n 3: 0.7020679853132146,\n 4: 0.9291282282393897,\n 8: 0.9326343488953125,\n 6: 0.9806641156445967,\n 9: 0.9773055520319955,\n 13: 0.9999999999999973,\n 10: 0.9999999999999973,\n 14: 0.9999999999999973}"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uGpfnzSqPFZA",
    "outputId": "8ac66227-21db-45f2-cf30-4fc333dfe83b"
   },
   "source": [
    "# Show policy\n",
    "policy"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: 2, 1: 2, 2: 1, 3: 0, 4: 1, 8: 2, 6: 1, 9: 2, 13: 2, 10: 1, 14: 2}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 3: Assess Success Rate"
   ],
   "metadata": {
    "id": "JSf6iUui3nT2"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GOBtngINPN9F",
    "outputId": "a9e34c6d-850a-47e8-fe6e-9420c6b77e90"
   },
   "source": [
    "wins = 0\n",
    "losses = 0\n",
    "for i_episode in range(1000):\n",
    "  state = env.reset()\n",
    "  while state < 15:\n",
    "    action = policy.get(state)\n",
    "    # Show state and action to take\n",
    "    # print(f'State, action {state, action}')\n",
    "    state, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "      if reward > 0:\n",
    "        wins += 1\n",
    "      else:\n",
    "        losses += 1\n",
    "      break\n",
    "print(f'Success rate (wins): {wins/(wins+losses) *100}%')"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate (wins): 100.0%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "With slippery = TRUE: Success rate (wins): 18.9% ... 27.6%"
   ],
   "metadata": {
    "id": "fmb7DWltTHQk"
   }
  }
 ]
}